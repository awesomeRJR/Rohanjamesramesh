{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twittersentiment analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8p3Gt516vhZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_COLUMNS=['Id','Location','target','text']"
      ],
      "metadata": {
        "id": "cDfa-G08CNBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/twitter_validation.csv',encoding=\"ISO-8859-1\",names=DATASET_COLUMNS)\n",
        "df"
      ],
      "metadata": {
        "id": "4xRt9wbJ6_vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of data is', len(df))"
      ],
      "metadata": {
        "id": "aQ5uBep2ChuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df. shape"
      ],
      "metadata": {
        "id": "vZ6wbo8JCqe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "ol_YRzuTCu5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(df.isnull().any(axis=1))"
      ],
      "metadata": {
        "id": "7hbaMAGeC3gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'].unique()"
      ],
      "metadata": {
        "id": "nzjpurEVC-bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x='target', data=df)"
      ],
      "metadata": {
        "id": "h6C8Qwr0DHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df.index[(df[\"target\"] == \"Irrelevant\")],axis=0,inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "buIBBGcUHMCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wOMJMYsgpCCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "gx2bXg9G4udk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ZzBxSuxsHjOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Id','Location'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Hgp6NqrYHn5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "AxANz5eVIFM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['target'] = df['target'].map({'Positive':1,'Neutral':0,'Negative':-1})"
      ],
      "metadata": {
        "id": "tXNd5PhvIIdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "tLAn9IO6Jdjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ukrj53a8KJEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=df.text\n",
        "tweets"
      ],
      "metadata": {
        "id": "61VpjVwlKtkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import TweetTokenizer\n",
        "tk=TweetTokenizer()\n",
        "tweets=tweets.apply(lambda x:tk.tokenize(x)).apply(lambda x:\" \".join(x))\n",
        "tweets"
      ],
      "metadata": {
        "id": "sxwpo3xTK2Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=tweets.str.replace('[^a-zA-Z0-9]+',' ')\n",
        "tweets"
      ],
      "metadata": {
        "id": "7WI2wYslMOyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tweets=tweets.apply(lambda x:' '.join([w for w in word_tokenize(x) if len(w)>=3]))\n",
        "tweets"
      ],
      "metadata": {
        "id": "OLx2rITXPZX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "stemmer=SnowballStemmer('english')\n",
        "tweets=tweets.apply(lambda x:[stemmer.stem(i.lower()) for i in tk.tokenize(x)]).apply(lambda x:' '.join(x))"
      ],
      "metadata": {
        "id": "NzFingfsQR00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "3itv7RcZRMv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop=stopwords.words('english')\n",
        "tweets=tweets.apply(lambda x:[i for i in word_tokenize(x) if i not in stop]).apply(lambda x:' '.join(x))"
      ],
      "metadata": {
        "id": "rbZOQPcfSPY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets"
      ],
      "metadata": {
        "id": "IwYNMAnURRv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec=TfidfVectorizer()\n",
        "train_data_vec=vec.fit_transform(tweets)"
      ],
      "metadata": {
        "id": "mDjfhUmxS-v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_vec)"
      ],
      "metadata": {
        "id": "wYFJevaMTroq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_vec.shape"
      ],
      "metadata": {
        "id": "OiWntd3Btum7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['target'].values\n",
        "y"
      ],
      "metadata": {
        "id": "n1r6gBV1TrkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(train_data_vec,y,test_size=.3,random_state=42)"
      ],
      "metadata": {
        "id": "EyDMGBCvc04b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "svm_model=svm.SVC()\n",
        "NB_model=MultinomialNB()\n",
        "RF_model=RandomForestClassifier()\n",
        "AB_model=AdaBoostClassifier()\n",
        "lstmodel=[svm_model,NB_model,RF_model,AB_model]"
      ],
      "metadata": {
        "id": "Bzq7ZYWedo5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay,classification_report\n",
        "for i in lstmodel:\n",
        "  print(i)\n",
        "  i.fit(x_train,y_train)\n",
        "  y_pred=i.predict(x_test)\n",
        "  print(\"*****************************************************\")\n",
        "  print(classification_report(y_test,y_pred))\n",
        "  print(\"*****************************************************\")\n"
      ],
      "metadata": {
        "id": "8BC7xLTsfulA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = ['This is the first document.','This document is the second document.','And this is the third one.']\n",
        "vectorizer = TfidfVectorizer()\n",
        "Xnew = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "8XK0_Cl4WfdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba =svm_model.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "0vQ9kRC_W3x1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}